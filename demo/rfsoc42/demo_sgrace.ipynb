{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHSP6-RBOqCE",
    "outputId": "ae211408-a548-40e5-844e-d6fd1c03d525"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGRACE loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import Amazon\n",
    "import torch_geometric.transforms as T\n",
    "#from torch_geometric.nn import GATConv, GATv2Conv\n",
    "from decimal import Decimal\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/xilinx/jupyter_notebooks/sgrace_lib')\n",
    "\n",
    "import config\n",
    "from sgrace import init_SGRACE,GATConv_SGRACE, Relu_SGRACE\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# the node degree is calculate in adj and if a row has a node degree of zero then the features of the node are set to zero.\n",
    "# I thought that for deep quantization there will be more rows at zero but this is not the case. The normalization\n",
    "# seems to make the adj values higher and then after quantization there are still not zero. This is problaby not a bad thing since \n",
    "# nodes that are initially connected and then remove will hurt accuracy. \n",
    "# In summary quantization reduces the number of connections for a node but nodes with just a single connection remain connected. \n",
    "norm_adj = 1 #use normalize adjacency\n",
    "custom = 1\n",
    "full_graph = 0\n",
    "training = 1\n",
    "\n",
    "batch_value = 128 #not relevant in planetoid that is a single graph, relevant for Amazon\n",
    "num_epochs = 200 \n",
    "\n",
    "#gnn max size\n",
    "\n",
    "init_SGRACE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j11WiUr-PRH_",
    "outputId": "5f84abf9-05bc-4c62-95f1-5f80b603b0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: AmazonPhoto():\n",
      "====================\n",
      "Number of graphs: 1\n",
      "Number of features: 745\n",
      "Number of classes: 8\n",
      "\n",
      "Data(x=[7650, 745], edge_index=[2, 238162], y=[7650])\n",
      "=============================================================\n",
      "Number of nodes: 7650\n",
      "Number of edges: 238162\n",
      "Average node degree: 31.13\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "average node degree\n",
      "31.132287581699348\n",
      "Fill value\n",
      "4.960339683950122\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "transform = None\n",
    "if (full_graph==1):\n",
    " #dataset_sel = \"Pubmed\"\n",
    " dataset_sel = \"Cora\"\n",
    " #dataset_sel = \"Citeseer\"\n",
    " dataset = Planetoid(root=\"data/Planetoid\", name=dataset_sel, split=\"full\", transform=transform) #split = \"full\"\n",
    "else:\n",
    " #dataset_sel = 'Computers'\n",
    " dataset_sel = 'Photo'\n",
    " dataset = Amazon(root=\"data/Amazon\", name=dataset_sel, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "print(\"average node degree\")\n",
    "average_node_degree = data.num_edges / data.num_nodes\n",
    "print(average_node_degree)\n",
    "print(\"Fill value\")\n",
    "print(math.log2(average_node_degree))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gZ-l0npPIca",
    "outputId": "65b6d684-0e67-42f3-bf60-c98ec1d7338a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/pynq-venv/lib/python3.8/site-packages/torch_geometric/sampler/neighbor_sampler.py:50: UserWarning: Using '{self.__class__.__name__}' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "if (full_graph==1):\n",
    " from torch_geometric.loader import DataLoader\n",
    "\n",
    " train_loader = DataLoader(dataset, batch_size=batch_value, shuffle=True)\n",
    " test_loader = DataLoader(dataset, batch_size=batch_value, shuffle=False)\n",
    "\n",
    "else:\n",
    "\n",
    " from torch_geometric.loader import NeighborLoader\n",
    "\n",
    " data = dataset[0]\n",
    "    \n",
    " #standard\n",
    " data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    " data.train_mask[:data.num_nodes - 1000] = 1\n",
    "\n",
    " data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    " data.test_mask[data.num_nodes - 1000:data.num_nodes - 500] = 1\n",
    "\n",
    "\n",
    " train_loader = NeighborLoader(data, batch_size=batch_value, num_neighbors=[10] *1,input_nodes= data.train_mask,shuffle=False)\n",
    " test_loader = NeighborLoader(data, batch_size=batch_value, num_neighbors=[10] * 1,input_nodes= data.test_mask,shuffle=False)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (att): GATConv(745, 16, heads=1)\n",
      "  (conv2): GATConv(16, 16, heads=1)\n",
      "  (lin): Linear(in_features=16, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LeakyReLU\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "#from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        #self.emb = AtomEncoder(dataset.num_node_features)\n",
    "        self.att = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        #self.conv3 = GCNConv(hidden_channels, 16)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        #self.lin2 = Linear(16, 16)\n",
    "        #self.lin3 = Linear(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = x.float()\n",
    "        #x = self.emb(x)\n",
    "        rmult = time.time()\n",
    "        \n",
    "        #recorder = DataRecorder(rails['0V85'].power)\n",
    "        #print('CPU forward kernel on')\n",
    "        #with recorder.record(0.2): # Sample every 500 ms\n",
    "        #  amult = time.time()\n",
    "        #  for _ in range(10):\n",
    "        x = self.att(x, edge_index)\n",
    "\n",
    "        x = x.relu() \n",
    "        #dmult =  time.time()   \n",
    "         #if (config.profiling == 1):\n",
    "        #print(recorder.frame)\n",
    "        #x = self.att(x, edge_index)\n",
    "        #x = x.relu()        \n",
    "        #lrelu = LeakyReLU(0.1)\n",
    "        #x = lrelu(x)\n",
    "        if (config.profiling == 1):\n",
    "         print('conv1 layer timing : {:.5f}s'.format(time.time() - rmult))\n",
    "        rmult = time.time()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if (config.profiling == 1):\n",
    "         print('conv2 layer timing : {:.5f}s'.format(time.time() - rmult))\n",
    "        \n",
    "        #x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # 2. Readout layer\n",
    "        #x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = x.relu()\n",
    "        #x = lrelu(x)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        #x = self.lin2(x)\n",
    "        #x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = GAT(hidden_channels=config.hidden_channels)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT_PYNQ INIT\n",
      "GAT_PYNQ(\n",
      "  (att2): GATConv_SGRACE (745 -> 16)\n",
      "  (conv22): GATConv_SGRACE (16 -> 16)\n",
      "  (reluh): Relu_SGRACE()\n",
      "  (lin): Linear(in_features=16, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "from torch.nn import LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_remaining_self_loops,add_self_loops,sort_edge_index,degree\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def sym_norm(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    fill_value = 1 if not improved else 2\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    row, col = edge_index\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "    return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "def sym_norm2(edge_index, num_nodes, edge_weight=None, improved=False, dtype=None):\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "    \n",
    "    # Calculate node degrees\n",
    "    node_degrees = degree(edge_index[1], num_nodes=num_nodes)\n",
    "\n",
    "    #print('max_degree')\n",
    "    #print(torch.max(node_degrees))\n",
    "\n",
    "  \n",
    "    \n",
    "    fill_value = math.trunc(math.log2(average_node_degree)) if not improved else 2\n",
    "    \n",
    "    #print(\"fill value\")\n",
    "    #print(fill_value)\n",
    "    #fill_value = torch.max(node_degrees) if not improved else 2\n",
    "    #fill_value = 1 if not improved else 2 #32\n",
    "\n",
    "    \n",
    "    #edge_weight = torch.zeros((edge_index.size(1), ), dtype=dtype, device=edge_index.device)\n",
    "    \n",
    "    #print(\"edge index\")\n",
    "    #print(edge_index)\n",
    "    edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)\n",
    "    \n",
    "    edge_index, edge_weight = sort_edge_index(edge_index, edge_weight) #make sure that self loops are in order\n",
    "    \n",
    "    row, col = edge_index\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    \n",
    "    return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "\n",
    "\n",
    "class GAT_PYNQ(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, hidden_channels,head_count):\n",
    "        super(GAT_PYNQ, self).__init__()\n",
    "        print(\"GAT_PYNQ INIT\")\n",
    "        #torch.manual_seed(12345)\n",
    "        \n",
    "\n",
    "        #self.att1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.att2 = GATConv_SGRACE(dataset.num_node_features, hidden_channels,head_count,dropout=0.1, alpha=0.2, concat=False)\n",
    "\n",
    "        #self.conv21 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv22 = GATConv_SGRACE(hidden_channels*head_count, hidden_channels,1)\n",
    "        \n",
    "        self.reluh = Relu_SGRACE()\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        \n",
    "   \n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        if (config.profiling==1):\n",
    "         ptime = time.time()\n",
    "            \n",
    "        if(config.profiling==1):\n",
    "         vtime = time.time();  \n",
    "        \n",
    "       \n",
    "         #print(\"Normalizing adjacency\")\n",
    "      #global adj\n",
    "        #adj = to_dense_adj(edge_index, edge_attr=norm)\n",
    "        #adj=torch.squeeze(adj)\n",
    "\n",
    "        #quantize adj\n",
    "\n",
    "         \n",
    "        #global pynq_adj \n",
    "        #pynq_adj = adj._to_sparse_csr()\n",
    "        \n",
    "\n",
    "        \n",
    "        edge_index, norm = sym_norm2(edge_index,x.size(0),improved=False)\n",
    "        \n",
    "        #global adj\n",
    "        adj = torch.sparse_coo_tensor(edge_index, norm) \n",
    "        \n",
    "        dense = 0\n",
    "        relu = 1\n",
    "        if (config.profiling==1):\n",
    "         fmult = time.time()\n",
    "        \n",
    "        #if (config.acc_deep==0):\n",
    "        # x = self.att1(relu,x,edge_index)\n",
    "        #else:\n",
    "        x = self.att2(config.compute_attention,dense,relu,x,edge_index,norm,adj)\n",
    "        \n",
    "        #print(\"out form first layer\")\n",
    "        #print(x)\n",
    "        \n",
    "        if (config.profiling == 1):\n",
    "         print('L1 layer time: {:.5f}ms'.format(1000*(time.time() - fmult)))\n",
    "        \n",
    "        if (config.profiling==1):\n",
    "         fmult = time.time()\n",
    "        x = self.reluh(x) #enable this to unmerge relu and take into account that relu is done in hardware \n",
    "        dense = 1 #hardwware execution mode for layer 2. 1 => fea dense\n",
    "\n",
    "        if(config.min_output==0):\n",
    "         print(\"SECOND LAYER ON\")\n",
    "\n",
    "        ######dense X\n",
    "        #xaux = x.detach().numpy()\n",
    "\n",
    "        #if(config.hardware_quantize == 0):\n",
    "        # support_xaux = quantization_uqbits(xaux,f_s2,f_z2,f_qbits) * (2**f_align)\n",
    "        #else:\n",
    "        # support_xaux = xaux\n",
    "        \n",
    "        #if(config.min_output==0):  \n",
    "        # print(\"Second layer quantize features sparsity\")\n",
    "        # isSparse(support_xaux, support_xaux.shape[0],support_xaux.shape[1])\n",
    "        #print(xaux)\n",
    "\n",
    "        #print(support_xaux)\n",
    "        #values_fea_buffer[0:(x.shape[0]*x.shape[1])] = (support_xaux.reshape(1,x.shape[0]*x.shape[1])) * (1<<f_align)\n",
    "        #config.values_fea_buffer[0:(x.shape[0]*x.shape[1])] = (support_xaux.reshape(1,x.shape[0]*x.shape[1]))# * (2**f_align) #cuidado    \n",
    "      \n",
    "   \n",
    "        relu = 0\n",
    "    \n",
    "        if (config.profiling == 1):\n",
    "         print('Relu time: {:.5f}ms'.format(1000*(time.time() - fmult)))\n",
    "\n",
    "\n",
    "        if (config.profiling == 1):\n",
    "         fmult = time.time()\n",
    " \n",
    "        #if (config.acc_deep==0):\n",
    "        # x = self.conv21(x,edge_index)\n",
    "        #else:\n",
    "        x = self.conv22(config.compute_attention,dense,relu,x,edge_index,norm,adj)\n",
    "        \n",
    "\n",
    "        if (config.profiling == 1):\n",
    "         print('L2 layer time: {:.5f}ms'.format(1000*(time.time() - fmult)))\n",
    "\n",
    "\n",
    "        # 2. Readout layer\n",
    "        if (config.profiling == 1):\n",
    "         fmult = time.time()\n",
    "        x = x.float()\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "  \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        #print(x.shape)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "\n",
    "\n",
    "        if (config.profiling == 1):\n",
    "         print('Readout time: {:.5f}ms'.format(1000*(time.time() - fmult)))\n",
    "        #print(x)\n",
    "        \n",
    "        if (config.profiling == 1):\n",
    "          print('Model time {:.5f}ms'.format(1000*(time.time() - ptime)))\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GAT_PYNQ(config.hidden_channels,config.head_count)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HvhgQoO8Svw4",
    "outputId": "4d17af35-85c8-4bcd-8e16-c514fd2ff714",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT_PYNQ INIT\n",
      "Using low learning rate\n"
     ]
    }
   ],
   "source": [
    "#from IPython.display import Javascript\n",
    "#display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "from torch_geometric.utils.convert import to_scipy_sparse_matrix\n",
    "from scipy import sparse\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "\n",
    "if (custom==0):\n",
    "  model = GAT(config.hidden_channels)\n",
    "else:        \n",
    "  model = GAT_PYNQ(config.hidden_channels,config.head_count)\n",
    "  #model_path = \"models/model_\" + dataset_sel + \"_fp.ptx\" #load best model\n",
    "  model_path = \"models/model_\" + dataset_sel + \"_8bit.ptx\" #load best model\n",
    "  model.load_state_dict(torch.load(model_path),strict=True)\n",
    "    \n",
    "  #Final results. The different learning rates are very important for different quantizations.  \n",
    "  #for 8-4 bit \n",
    "  #optimizer = torch.optim.Adam(model.parameters(),  lr=0.005)\n",
    "  #for 2-1 bit optimizer \n",
    "  #optimizer = torch.optim.Adam(model.parameters(),  lr=0.05)\n",
    "  #optimizer = torch.optim.Adam(model.parameters(),  lr=0.1)\n",
    "    \n",
    "  if(config.w_qbits>2 or config.acc_deep==0):\n",
    "    print(\"Using low learning rate\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "  else:\n",
    "    #for 2-1 bit optimizer\n",
    "    print(\"Using high learning rate\")\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.001) #with load state use low reaning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(),  lr=0.1) #GAT benefits from this ?\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "       \n",
    "    \n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  #criterion = torch.nn.NLLLoss()\n",
    "\n",
    "def zero_count(array, n): \n",
    " counter = 0\n",
    " # Count number of zeros\n",
    " # in the matrix\n",
    " for i in range(0, n):\n",
    "  if (array[i] == 0):\n",
    "   counter = counter + 1\n",
    " print(\"total values \",n)\n",
    " print(\"zero values \",counter)\n",
    " return (counter > ((n) // 2))\n",
    "\n",
    "\n",
    "def accuracy(x, labels, dataset_sel):\n",
    "\n",
    " x1 = np.equal(x, labels)\n",
    " x2 = np.sum(x1)\n",
    "\n",
    " if isinstance(x, list):\n",
    "     acc = x2 / len(x)\n",
    " else:\n",
    "     acc = x2 / x.size\n",
    " return acc\n",
    "\n",
    "\n",
    "def train():\n",
    "  model.train()\n",
    "  for bid, batch in enumerate(train_loader):\n",
    "       batchsize = batch.x.shape[0]\n",
    "  #for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "       tmult = time.time()\n",
    "       #global num_nodes_h\n",
    "       #num_nodes_h = batchsize\n",
    "       if (custom==0):\n",
    "        #print(\"Running TRAIN with full precision\")\n",
    "        #out = model(data.x, data.edge_index)  # Perform a single forward pass. \n",
    "        out = model(batch.x, batch.edge_index)  # Perform a single forward pass. \n",
    "       else:\n",
    "        #out = model(data.x, data.edge_index)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        if (config.profiling == 1):\n",
    "         print('Forward train time: {:.5f}s'.format(time.time() - tmult))\n",
    "       #loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "       loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "\n",
    "       tmult = time.time()\n",
    "       loss.backward()  # Derive gradients.\n",
    "       if (config.profiling == 1):\n",
    "        print('backward time: {:.5f}s'.format(time.time() - tmult))\n",
    "       optimizer.step()  # Update parameters based on gradients.\n",
    "       optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader,split):\n",
    "   model.eval()\n",
    "     \n",
    "   preds_l = []\n",
    "   labels_l = []\n",
    "\n",
    "   for bid, batch in enumerate(loader):  # Iterate in batches over the training/test dataset.\n",
    "   #    batchsize = batch.x.shape[0]\n",
    "   #    global num_nodes_h \n",
    "   #    num_nodes_h = batchsize\n",
    "       #print(\"graph size is \", batch.x.shape[0]) \n",
    "   #for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "       if (config.profiling==1):\n",
    "        tmult = time.time()\n",
    "       if (custom==0):\n",
    "        #out = model(data.x, data.edge_index) \n",
    "        out = model(batch.x, batch.edge_index) \n",
    "       #print(\"Test\")\n",
    "       else:\n",
    "        #out = model(data.x, data.edge_index)\n",
    "        out = model(batch.x, batch.edge_index) \n",
    "       #print(out)\n",
    "       if (config.profiling == 1):\n",
    "        print('Forward test time: {:.5f}s'.format(time.time() - tmult))\n",
    "            \n",
    "            \n",
    "       if (split == \"train\"):\n",
    "         preds_l.append(out[batch.train_mask].detach().numpy())\n",
    "         labels_l.append(batch.y[batch.train_mask].detach().numpy())\n",
    "       elif (split == \"test\"):\n",
    "         preds_l.append(out[batch.test_mask].detach().numpy())\n",
    "         labels_l.append(batch.y[batch.test_mask].detach().numpy()) \n",
    "       preds = np.argmax(np.concatenate(preds_l), axis=1)\n",
    "        \n",
    "       #if (split == \"train\"):\n",
    "       # preds_l.append(out[data.train_mask].detach().numpy())\n",
    "       # labels_l.append(data.y[data.train_mask].detach().numpy())\n",
    "       #elif (split == \"test\"):\n",
    "       # preds_l.append(out[data.test_mask].detach().numpy())\n",
    "       # labels_l.append(data.y[data.test_mask].detach().numpy()) \n",
    "       #preds = np.argmax(np.concatenate(preds_l), axis=1)\n",
    "\n",
    "    \n",
    "   pred_acc = accuracy(preds, np.concatenate(labels_l), dataset_sel)\n",
    "            \n",
    "   return pred_acc  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "print('Running inference only with train and test data sets')\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "   amult = time.time()\n",
    "   test_acc = test(test_loader,\"test\") \n",
    "   train_acc = test(train_loader,\"train\") \n",
    "   print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Time: {(time.time() - amult):.4f}')\n",
    "\n",
    "#quit()\n",
    "#exit()\n",
    "#raise SystemExit(\"Stop right there!\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if (training==1):\n",
    "    \n",
    " print(\"Training\")\n",
    "    \n",
    " best_acc = 0\n",
    " best_epoch = 0\n",
    " for epoch in range(num_epochs):\n",
    "       \n",
    "  amult = time.time()\n",
    "    #print(\"Running TRAIN\")\n",
    "  train()\n",
    "  #print(\"Running TEST\")\n",
    "  #train_acc = test(train_loader,\"train\") #remove to speed up\n",
    "  #train_acc = 0\n",
    "  test_acc = test(test_loader,\"test\") \n",
    "    \n",
    "  if (test_acc > best_acc):\n",
    "   best_acc = test_acc\n",
    "   best_epoch = epoch\n",
    "   model_path = \"models/model_\" + dataset_sel + \".ptx\" #save best model\n",
    "   torch.save(model.state_dict(), model_path)\n",
    "\n",
    "  print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}, Time: {(time.time() - amult):.4f}')\n",
    "   \n",
    " print(' ')\n",
    "  \n",
    " print('Best accuracy: ', best_acc)\n",
    " print('Best epoch: ', best_epoch)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "   amult = time.time()\n",
    "   test_acc = test(test_loader,\"test\") \n",
    "   train_acc = test(train_loader,\"train\") \n",
    "   print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Time: {(time.time() - amult):.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import sgrace\n",
    "\n",
    "#print(\"Number of values in weight matrix\")\n",
    "#print(dataset.num_features*16)\n",
    "#print(B_buffer[0:(dataset.num_features*16)])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "##dense_adj = adj.to_dense()\n",
    "#dense_adj[dense_adj > 0] = 1\n",
    "##plt.imshow(dense_adj, cmap='binary')\n",
    "##plt.colorbar()\n",
    "##plt.title(\"Adjacency Matrix\")\n",
    "##plt.show()\n",
    "\n",
    "##print(\"positive, zero and negative adj counts\")\n",
    "##pos_a = (dense_adj > 0).sum()\n",
    "##zero_a = (dense_adj == 0).sum()\n",
    "##neg_a = (dense_adj < 0).sum()\n",
    "##print(pos_a,\" \",zero_a,\" \",neg_a) \n",
    "\n",
    "##print(\"positive, zero and negative feature counts\")\n",
    "##pos_f = (support_x > 0).sum()\n",
    "##zero_f = (support_x == 0).sum()\n",
    "##neg_f = (support_x < 0).sum()\n",
    "##print(pos_f,\" \",zero_f,\" \",neg_f) \n",
    "    \n",
    "\n",
    "##mybins = []\n",
    "##for k in range(0,(2**(f_qbits))+2):\n",
    "#for k in range(-128,128):    \n",
    "#for k in range(-8,7):\n",
    "    #print(k) \n",
    "##    mybins += [k]\n",
    "#mybins = [-2, -1, 0, 1]\n",
    "#y = B_buffer[0:(dataset.num_features*16)]\n",
    "##y = quantization_uqbits(norm,a_s,a_z,a_qbits)\n",
    "#print('max_degree')\n",
    "#print(torch.max(node_degrees))\n",
    "#y = model.att2.weight.data*128\n",
    "#y = y.reshape(1,adj_size)\n",
    "##print(\"Adjacency values\")\n",
    "##print(y)\n",
    "##y_q = y #quantization_qbits(y,w_s,w_z,w_qbits)\n",
    "\n",
    "#y = B_buffer[0:(dataset.num_features*16)]\n",
    "\n",
    "\n",
    "#print(\"Number of selected value\")\n",
    "#print(np.count_nonzero(y == -8))\n",
    "\n",
    "##plt.figure(figsize=(10, 10))\n",
    "##plt.xlabel('Adjacency')\n",
    "##plt.ylabel('Frequency')\n",
    "#counts, bins, bars = plt.hist(y, bins=256)\n",
    "##counts, bins, bars = plt.hist(y_q,mybins)\n",
    "#plt.yticks(np.arange(0, 12000, step=500))\n",
    "#plt.xticks(mybins,horizontalalignment='center',fontsize=12,rotation=90)\n",
    "#plt.gca().xaxis.set_major_locator(MaxNLocator(nbins=32))\n",
    "\n",
    "#plt.xticks(mybins,horizontalalignment='right',fontsize=12,rotation=90)\n",
    "##plt.xticks(mybins[::8],horizontalalignment='right',fontsize=12,rotation=90)\n",
    "#plt.xticks(range(0,2**(a_qbits)+2,1)[::8],rotation=90)\n",
    "##plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mybins = []\n",
    "for k in range((-2**(config.w_qbits-1)),(2**(config.w_qbits-1))+2):\n",
    "#for k in range(-128,128):    \n",
    "#for k in range(-8,7):\n",
    "    #print(k) \n",
    "    mybins += [k]\n",
    "#mybins = [-2, -1, 0, 1]\n",
    "#y = B_buffer[0:(dataset.num_features*16)]\n",
    "y = sgrace.quantization_qbits(model.att2.weight.data,sgrace.w_s,sgrace.w_z,config.w_qbits)\n",
    "#y = model.att2.weight.data\n",
    "#y = model.att2.weight.data*128\n",
    "y = y.reshape(1,dataset.num_features*16)\n",
    "#torch.set_printoptions(threshold=np.inf)\n",
    "#print(\"Weights L1 bins\")\n",
    "#print(mybins)\n",
    "#print(\"Weights L1 float values before q\")\n",
    "#print(model.att2.weight.data)\n",
    "#print(\"Weights L1 float values after q\")\n",
    "#y_d = y*w_s_o/(2**frac_bits_o)\n",
    "#y_d = y*w_s\n",
    "#print(y_d)\n",
    "y_q = y #quantization_qbits(y,w_s,w_z,w_qbits)\n",
    "\n",
    "#y = B_buffer[0:(dataset.num_features*16)]\n",
    "\n",
    "\n",
    "#print(\"Number of selected value\")\n",
    "#print(np.count_nonzero(y == -8))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlabel('Weights L1')\n",
    "plt.ylabel('Frequency')\n",
    "#counts, bins, bars = plt.hist(y, bins=256)\n",
    "counts, bins, bars = plt.hist(y_q,mybins)\n",
    "#plt.yticks(np.arange(0, 12000, step=500))\n",
    "#plt.xticks(mybins,horizontalalignment='center',fontsize=12,rotation=90)\n",
    "plt.xticks(mybins[::8],horizontalalignment='right',fontsize=12,rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#plt.bar(y_q,mybins, align='center')\n",
    "#plt.gca().set_xticks(labels)\n",
    "#plt.show()\n",
    "\n",
    "print('max/min weight')\n",
    "print(torch.max(model.att2.weight.data))\n",
    "print(torch.min(model.att2.weight.data))\n",
    "\n",
    "mybins = []\n",
    "for k in range((-2**(config.w_qbits-1)),(2**(config.w_qbits-1))+2):\n",
    "#for k in range(-128,128):\n",
    "#for k in range(-8,7):\n",
    "    #print(k)  \n",
    "    mybins += [k]\n",
    "#mybins = [-2, -1, 0, 1]\n",
    "#y = D_buffer[0:(num_nodes_h*16)]\n",
    "y = quantization_qbits(model.conv22.weight.data,w_s2,w_z2,config.w_qbits)\n",
    "#y = model.conv22.weight.data\n",
    "y = y.reshape(1,16*16)\n",
    "print(\"Weights L2 values\")\n",
    "print(y)\n",
    "print(mybins)\n",
    "y_q = y #quantization_qbits(y,f_s,f_z,f_qbits)\n",
    "\n",
    "#y = B_buffer[0:(dataset.num_features*16)]\n",
    "\n",
    "\n",
    "#print(\"Number of selected value\")\n",
    "#print(np.count_nonzero(y == -8))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "#plt.xlabel('Output  values')\n",
    "plt.xlabel('Weights L2')\n",
    "plt.ylabel('Frequency')\n",
    "#counts, bins, bars = plt.hist(y, bins=256)\n",
    "counts, bins, bars = plt.hist(y_q,mybins)\n",
    "#plt.yticks(np.arange(0, 12000, step=500))\n",
    "#plt.xticks(mybins,horizontalalignment='center',fontsize=12,rotation=90)\n",
    "plt.xticks(mybins[::8],horizontalalignment='center',fontsize=12,rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print('max/min weight')\n",
    "print(torch.max(model.conv22.weight.data))\n",
    "print(torch.min(model.conv22.weight.data))\n",
    "\n",
    "print(\"MAX FEA INTERNAL VALUE layer 1\", cur_max_fea)\n",
    "print(\"MAX FEA INTERNAL VALUE layer 2\", cur_max_fea2)\n",
    "print(\"Use this to adjust your hardware ITYPE width\")\n",
    "print(\"Current attention is:\")\n",
    "print(attention_buffer)\n",
    "#print(bins)\n",
    "#print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecJCNRmT2RsF"
   },
   "source": [
    "## (Optional) Exercise\n",
    "\n",
    "Can we do better than this?\n",
    "As multiple papers pointed out ([Xu et al. (2018)](https://arxiv.org/abs/1810.00826), [Morris et al. (2018)](https://arxiv.org/abs/1810.02244)), applying **neighborhood normalization decreases the expressivity of GNNs in distinguishing certain graph structures**.\n",
    "An alternative formulation ([Morris et al. (2018)](https://arxiv.org/abs/1810.02244)) omits neighborhood normalization completely and adds a simple skip-connection to the GNN layer in order to preserve central node information:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_v^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{w \\in \\mathcal{N}(v)} \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "This layer is implemented under the name [`GraphConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv) in PyTorch Geometric.\n",
    "\n",
    "As an exercise, you are invited to complete the following code to the extent that it makes use of PyG's `GraphConv` rather than `GCNConv`.\n",
    "This should bring you close to **82% test accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNgkR8SRaU_P"
   },
   "source": [
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = ...  # TODO\n",
    "        self.conv2 = ...  # TODO\n",
    "        self.conv3 = ...  # TODO\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GNN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs5D713Ia_Sv"
   },
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GNN(hidden_channels=64)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tBMhOrq4JKw"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this chapter, you have learned how to apply GNNs to the task of graph classification.\n",
    "You have learned how graphs can be batched together for better GPU utilization, and how to apply readout layers for obtaining graph embeddings rather than node embeddings.\n",
    "\n",
    "In the next session, you will learn how you can utilize PyTorch Geometric to let Graph Neural Networks scale to single large graphs.\n",
    "\n",
    "[Next: Scaling Graph Neural Networks](https://colab.research.google.com/drive/1XAjcjRHrSR_ypCk_feIWFbcBKyT4Lirs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
